### 进度

1. 已完成
- 前期数据处理和降维模块
- 后期模型评估模块
- 用liblinear实现多元logistic回归

2. 存在问题
- 模型训练慢，看起来像是算法不收敛
- tf-idf的阈值设置需要调整

### 数据分析

- 真实分类多

描述性统计结果（耗时44.392799139s）：

```
basic_statistics(min_val=1, max_val=1344, median=20, mean_val=27.61187490936296)
```
共1475666个不同的y，最少的覆盖1个样本，最大的覆盖1344（0.1%）个样本，中位数是20，均值是27.6，呈现左偏，但不是太严重。

- 特征维度高

特征维度近20万。这些特征都是词条，可以设合理的tf-idf阈值来降维。

- 真实分类所包含叶结点的关系
据官方文档，真实分类只包含层次关系的叶结点。某条分类中的若干个叶结点可能属于同一个父结点。这样对于那些特别稀疏的叶结点可以合并成它们的父结点，作为预测目标。即先预测父结点，再对相同父结点的样本预测子结点。


### 程序效率

1. 问题
将2365436行读入内存并分割x, y耗时过长，理论上时间复杂度是 O(m*p) , 其中m是样本数，p是平均每条样本包含的特征数。liblinear的svm_read_problem()方法用的也是类似的读入逻辑，估计用python直接将200万行的文件直接读入内存必然会效率低下的。又或者是因为我的机器内存太低？
在一个完整的数据挖掘流程中，需要优化的不仅是解算法最优化的过程，因为这个过程只占其中很小一部分，很大一部分是在数据的读写和处理，包括特征处理等。
以下是在完整训练数据上抽取前k条来测试数据处理（提取y和x）的耗时：

2. 改进方法
不要一次处理太多问题，将TrainData的各个逻辑解开。
改进后的耗时还可以接受嗯：

- 10000: 1.93s
- 100000: 14s
- 1000000: 160s
- 2365436: 347s

### 模型效果
取数据的前100行作为训练集，第101到200行作为测试集。
在训练集用tf-idf作降维，用降维后的x作为分类器的输入数据，tf-idf阈值是0.1 
首先尝试的是liblinear的线性分类器，具体参数是'-s 0 -c 1'，即正则化系数为1，模型为带l2正则项的logistic回归，用原问题作为最优化求解的目标函数。
训练集上的整体准确率达到96.9697%， 即99个样本中有96个分类正确，macro precision和macro recall分别是0.535和0.532.

取完整数据的前1000,000行（共有特征197496个），用tf-idf进行降维后，余下10001个样本点，共9834个特征。
将10001个样本点切分为训练集和测试集，其中训练集9635条记录，同样是用logistic回归，l2正则项系数为1。
这时的训练集准确率为0。04％，即9635个训练样本中只有4个预测正确。

### 将训练数据取前10000行时，报异常，内存泄漏
Python(1407,0x7fff7b156000) malloc: *** mach_vm_map(size=18446744068723740672) failed (error code=3)
*** error: can't allocate region
*** set a breakpoint in malloc_error_break to debug

### 转成2元分类问题之后
取前1000行做实验，降维后只有266行，其中212行被分为训练样本，54行被分为测试样本。
训练样本中有4个正样本，测试样本中没有正样本。
这时候模型会倾向于将样本都预测为负样本，以logistic回归为例，如果样本的特征值都是正数，那么这时候训练出来的参数向量会大多数分量都是负值或0，这样就会使得sigmoid后的w*x非常小，接近于0，即预测值为－1
这样做会使训练集和测试集上的效果都非常好，通常都在90％以上。
但这可以认为是训练样本不具有代表性，不能代表数据的真实分布，因此在算法求解时的目标函数"结构风险函数"中的一项"经验风险函数"无法很好地对期望风险函数进行估计，因此以其为目标函数迭代出来的解也是与真实值相差甚远的。

但当取前10万行时，效果似乎也还是不错的：

DataDesc(sample_size=100000, feature_size=197496, label_size=76595, train_size=0, test_size=0)
DataDesc(sample_size=59747, feature_size=49417, label_size=2, train_size=0, test_size=0)
DataDesc(sample_size=59747, feature_size=49417, label_size=2, train_size=47797, test_size=11950)

iter  1 act 1.082e+04 pre 1.011e+04 delta 8.669e+01 f 3.313e+04 |g| 3.926e+02 CG   8
iter  2 act 4.873e+02 pre 4.332e+02 delta 8.669e+01 f 2.231e+04 |g| 8.574e+01 CG   6
iter  3 act 4.622e+01 pre 4.060e+01 delta 8.669e+01 f 2.182e+04 |g| 2.495e+01 CG   4
iter  4 act 4.653e+00 pre 4.284e+00 delta 8.669e+01 f 2.178e+04 |g| 6.336e+00 CG   4
iter  5 act 1.235e-01 pre 1.201e-01 delta 8.669e+01 f 2.177e+04 |g| 9.526e-01 CG   3

训练样本中的正样本数：741

训练集上的分类效果：
Accuracy = 99.4058% (47513/47797) (classification)
(99.40582044898215, 0.023767182040713854, 0.7266794438081764)

测试集上的分类效果：
Accuracy = 93.1464% (11131/11950) (classification)
(93.14644351464435, 0.27414225941422593, 0.01120445052678307)

看来仅对单个label, 如24177来分类，效果还是不错的。那是不是说将所有的单label抽离出来作为正样本，训练 ```#label``` 个分类器
在预测时将各个样本被预测为正的label组合起来，就是它的真实label串？
接下来可以将label串拆成多个label，这样一条样本就对应新的多条样本，然后进行多元分类。但在这之前首先重构一下代码嗯。

既然各个label是叶结点的类别，那么它们之间应该不会有什么相关关系，除了它们同属于同一个内结点之外。所以将它们整体作为一个新的类别不会带来太多额外信息。还是先分割了再做分类。这样就不需要对label串做重新映射了。

在前1000个样本上测试多元分类，预测结果和macro precision / recall 分别是：
(0.045174537987679675, 0.04247946611909651)