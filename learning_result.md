### 进度

1. 已完成
- 前期数据处理和降维模块
- 后期模型评估模块
- 用liblinear实现多元logistic回归

2. 存在问题
- 模型训练慢，看起来像是算法不收敛
- tf-idf的阈值设置需要调整

### 数据分析

- 真实分类多

描述性统计结果（耗时44.392799139s）：

```
basic_statistics(min_val=1, max_val=1344, median=20, mean_val=27.61187490936296)
```
共1475666个不同的y，最少的覆盖1个样本，最大的覆盖1344（0.1%）个样本，中位数是20，均值是27.6，呈现左偏，但不是太严重。

- 特征维度高

特征维度近20万。这些特征都是词条，可以设合理的tf-idf阈值来降维。

- 真实分类所包含叶结点的关系
据官方文档，真实分类只包含层次关系的叶结点。某条分类中的若干个叶结点可能属于同一个父结点。这样对于那些特别稀疏的叶结点可以合并成它们的父结点，作为预测目标。即先预测父结点，再对相同父结点的样本预测子结点。


### 程序效率

1. 问题
将2365436行读入内存并分割x, y耗时过长，理论上时间复杂度是 O(m*p) , 其中m是样本数，p是平均每条样本包含的特征数。liblinear的svm_read_problem()方法用的也是类似的读入逻辑，估计用python直接将200万行的文件直接读入内存必然会效率低下的。又或者是因为我的机器内存太低？
在一个完整的数据挖掘流程中，需要优化的不仅是解算法最优化的过程，因为这个过程只占其中很小一部分，很大一部分是在数据的读写和处理，包括特征处理等。
以下是在完整训练数据上抽取前k条来测试数据处理（提取y和x）的耗时：

2. 改进方法
不要一次处理太多问题，将TrainData的各个逻辑解开。
改进后的耗时还可以接受嗯：

- 10000: 1.93s
- 100000: 14s
- 1000000: 160s
- 2365436: 347s

### 模型效果
取数据的前100行作为训练集，第101到200行作为测试集。
在训练集用tf-idf作降维，用降维后的x作为分类器的输入数据，tf-idf阈值是0.1 
首先尝试的是liblinear的线性分类器，具体参数是'-s 0 -c 1'，即正则化系数为1，模型为带l2正则项的logistic回归，用原问题作为最优化求解的目标函数。
训练集上的整体准确率达到96.9697%， 即99个样本中有96个分类正确，macro precision和macro recall分别是0.535和0.532.

### 将训练数据取前10000行时，报异常，内存泄漏
Python(1407,0x7fff7b156000) malloc: *** mach_vm_map(size=18446744068723740672) failed (error code=3)
*** error: can't allocate region
*** set a breakpoint in malloc_error_break to debug
